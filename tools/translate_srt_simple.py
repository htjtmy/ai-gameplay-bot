#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨Ollamaæˆ–å…¶ä»–æœ¬åœ°ç¿»è¯‘æœåŠ¡ç¿»è¯‘SRTå­—å¹•
ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨é¢„å®šä¹‰çš„ç¿»è¯‘æ˜ å°„å’Œå¯å‘å¼æ–¹æ³•
"""

import re
from pathlib import Path
from typing import List, Tuple

def parse_srt(file_path: str) -> List[Tuple[int, str, List[str]]]:
    """è§£æSRTæ–‡ä»¶ï¼Œè¿”å› (åºå·, æ—¶é—´æˆ³, å­—å¹•è¡Œåˆ—è¡¨)"""
    with open(file_path, 'r', encoding='utf-8-sig') as f:
        content = f.read()
    
    # å°†å†…å®¹åˆ†è§£ä¸ºå­—å¹•å—
    blocks = content.strip().split('\n\n')
    subtitles = []
    
    for block in blocks:
        lines = block.strip().split('\n')
        if len(lines) >= 3:
            try:
                seq_num = int(lines[0])
                timestamp = lines[1]
                subtitle_text = '\n'.join(lines[2:])
                subtitles.append((seq_num, timestamp, subtitle_text.split('\n')))
            except:
                continue
    
    return subtitles

def simple_translate(text: str) -> str:
    """ä½¿ç”¨ç®€å•è§„åˆ™å’Œç¼“å­˜è¿›è¡Œç¿»è¯‘"""
    # åŸºç¡€è¯æ±‡å’ŒçŸ­è¯­ç¿»è¯‘
    vocab = {
        # åŸºç¡€è¯
        'AI': 'AI',
        'the': 'è¿™',
        'is': 'æ˜¯',
        'and': 'å’Œ',
        'in': 'åœ¨',
        'to': 'åˆ°',
        'of': 'çš„',
        'that': 'é‚£',
        'it': 'å®ƒ',
        'a': 'ä¸€ä¸ª',
        'for': 'å¯¹äº',
        'or': 'æˆ–',
        'I': 'æˆ‘',
        'you': 'ä½ ',
        'he': 'ä»–',
        'we': 'æˆ‘ä»¬',
        'they': 'ä»–ä»¬',
        
        # é•¿çŸ­è¯­
        'artificial intelligence': 'äººå·¥æ™ºèƒ½',
        'machine learning': 'æœºå™¨å­¦ä¹ ',
        'neural network': 'ç¥ç»ç½‘ç»œ',
        'deep learning': 'æ·±åº¦å­¦ä¹ ',
        'large language model': 'å¤§å‹è¯­è¨€æ¨¡å‹',
        'foundation model': 'åŸºç¡€æ¨¡å‹',
        'transformer': 'å˜å‹å™¨/è½¬æ¢å™¨',
        'reinforcement learning': 'å¼ºåŒ–å­¦ä¹ ',
        'open source': 'å¼€æº',
        'safety': 'å®‰å…¨',
        'alignment': 'å¯¹é½',
        'AGI': 'é€šç”¨äººå·¥æ™ºèƒ½',
        'scaling': 'æ‰©å±•',
        'compute': 'è®¡ç®—',
        'data': 'æ•°æ®',
        'training': 'è®­ç»ƒ',
        'model': 'æ¨¡å‹',
        'system': 'ç³»ç»Ÿ',
        'human': 'äººç±»',
        'intelligence': 'æ™ºèƒ½',
        'learning': 'å­¦ä¹ ',
        'knowledge': 'çŸ¥è¯†',
        'world': 'ä¸–ç•Œ',
        'problem': 'é—®é¢˜',
        'solution': 'è§£å†³æ–¹æ¡ˆ',
        'research': 'ç ”ç©¶',
        'science': 'ç§‘å­¦',
        'technology': 'æŠ€æœ¯',
        'future': 'æœªæ¥',
        
        # å¸¸ç”¨çŸ­è¯­
        'thank you': 'è°¢è°¢ä½ ',
        'good morning': 'æ—©ä¸Šå¥½',
        'good afternoon': 'ä¸‹åˆå¥½',
        'how are you': 'ä½ å¥½å—',
        'very well': 'éå¸¸å¥½',
        'I think': 'æˆ‘è®¤ä¸º',
        'I believe': 'æˆ‘ç›¸ä¿¡',
        'I want': 'æˆ‘æƒ³è¦',
        'I need': 'æˆ‘éœ€è¦',
        'you are': 'ä½ æ˜¯',
        'you can': 'ä½ å¯ä»¥',
        'you have': 'ä½ æœ‰',
        'would be': 'å°†æ˜¯',
        'can be': 'å¯ä»¥æ˜¯',
        'should be': 'åº”è¯¥æ˜¯',
        'might be': 'å¯èƒ½æ˜¯',
        'may not': 'å¯èƒ½ä¸',
        'do not': 'ä¸',
        'does not': 'ä¸',
        'will not': 'ä¸ä¼š',
        'cannot': 'ä¸èƒ½',
        
        # é—®é¢˜è¯
        'what': 'ä»€ä¹ˆ',
        'why': 'ä¸ºä»€ä¹ˆ',
        'when': 'ä»€ä¹ˆæ—¶å€™',
        'where': 'å“ªé‡Œ',
        'who': 'è°',
        'how': 'æ€æ ·',
        'which': 'å“ªä¸€ä¸ª',
        'question': 'é—®é¢˜',
        'answer': 'ç­”æ¡ˆ',
        
        # ç‰¹æ®Šé¡¹
        'Noah': 'è¯ºäºš',
        'Harari': 'å“ˆæ‹‰é‡Œ',
        'Yoshua': 'çº¦ä¹¦äºš',
        'Eric': 'åŸƒé‡Œå…‹',
        'Eugene': 'å°¤é‡‘',
        'Davos': 'è¾¾æ²ƒæ–¯',
        'Korea': 'éŸ©å›½',
        'Europe': 'æ¬§æ´²',
        'US': 'ç¾å›½',
        'China': 'ä¸­å›½',
        'GPT': 'GPT',
    }
    
    # è½¬æ¢ä¸ºå°å†™è¿›è¡ŒåŒ¹é…
    text_lower = text.lower()
    
    # å°è¯•é•¿çŸ­è¯­åŒ¹é…ï¼ˆä»é•¿åˆ°çŸ­ï¼‰
    phrases = sorted(vocab.keys(), key=len, reverse=True)
    for phrase in phrases:
        if phrase in text_lower:
            pattern = re.compile(re.escape(phrase), re.IGNORECASE)
            text = pattern.sub(vocab[phrase], text)
    
    return text

def write_srt(file_path: str, subtitles: List[Tuple[int, str, List[str]]]):
    """å†™å…¥SRTæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for i, (seq_num, timestamp, subtitle_lines) in enumerate(subtitles):
            f.write(f"{seq_num}\n")
            f.write(f"{timestamp}\n")
            for line in subtitle_lines:
                f.write(f"{line}\n")
            if i < len(subtitles) - 1:
                f.write("\n")

def main():
    input_file = r'd:\Users\Source\Ai-Gameplay-Bot\logs\en.srt'
    output_file = r'd:\Users\Source\Ai-Gameplay-Bot\logs\zh-cn.srt'
    
    input_path = Path(input_file)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    print(f"ğŸ“– æ­£åœ¨è¯»å–: {input_file}")
    print(f"â³ è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ...")
    
    try:
        subtitles = parse_srt(input_file)
        print(f"âœ… å·²è¯»å– {len(subtitles)} æ¡å­—å¹•")
        
        print(f"ğŸ”„ æ­£åœ¨ç¿»è¯‘...")
        translated_subs = []
        for seq_num, timestamp, subtitle_lines in subtitles:
            translated_lines = [simple_translate(line) for line in subtitle_lines]
            translated_subs.append((seq_num, timestamp, translated_lines))
        
        print(f"âœï¸  æ­£åœ¨å†™å…¥: {output_file}")
        write_srt(output_file, translated_subs)
        
        print(f"\nâœ… ç¿»è¯‘å®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»å­—å¹•æ¡æ•°: {len(translated_subs)}")
        print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"   - ç¼–ç : UTF-8")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
